{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "Assume the temperature in City 1 is  X1 a Gaussian random variable with mean =60 and  var = 10, and that the temperature of City 2 is X2 is a Gaussian random variable with mean=90 and var = 20.\n",
    "Moreover, we know that the covariance between X1 and X2 is 100. Today, we have observed that the temperature in City 2 is 75. What is the probability that the new temperature in the City 1 is bigger than 56.25?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.25\n"
     ]
    }
   ],
   "source": [
    "# Compute conditional expectation\n",
    "mu_1, sigma_1 = 60, 10\n",
    "mu_2, sigma_2 = 90, 20\n",
    "cov = 100\n",
    "observed_2 = 75\n",
    "\n",
    "conditional_mu = mu_1 + cov/(sigma_2**2) * (observed_2 - mu_2)\n",
    "print(conditional_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Entropy of a uniform distribution.\n",
    "\n",
    "Let p(x) = 0.25 for  x belongs to X =[1,2,3,4]\n",
    "\n",
    "Compute the entropy of this distribution in bits (log base 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Compute entropy of the distribution, H(X)\n",
    "import numpy as np\n",
    "p = 0.25\n",
    "X = np.array([1,2,3,4])\n",
    "\n",
    "h_X = np.sum([-p*np.log2(p) for _ in X])\n",
    "print(h_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Entropy of a joint distribution.\n",
    "\n",
    "Let p(x,y) be a joint distribution over  x belongs to {0,1} and y belongs to {0,1} such that:\n",
    "\n",
    "p(0,0)=0.5\n",
    "\n",
    "p(0,1)=0\n",
    "\n",
    "p(1,0)=0.25\n",
    "\n",
    "p(1,1) = 0.25\n",
    "\n",
    "What is the entropy of this distribution in bits? Note that:\n",
    "\n",
    "limit (z->0) zlogz ->0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "# Compute the entropy of the distribution\n",
    "\n",
    "p_XY = np.array([0.5, 0, 0.25, 0.25])\n",
    "h_XY = np.sum([-p * np.log2(p) for p in p_XY if p != 0])\n",
    "\n",
    "print(h_XY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional entropy.\n",
    "\n",
    "Let p(x,y) be a joint distribution over  x belongs to {0,1} and y belongs to {0,1} such that:\n",
    "\n",
    "p(0,0)=0.5\n",
    "\n",
    "p(0,1)=0\n",
    "\n",
    "p(1,0)=0.25\n",
    "\n",
    "p(1,1) = 0.25\n",
    "\n",
    "What is the conditional entropy of X given Y in bits? please provide your answers to three significant figures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6887218755408672\n"
     ]
    }
   ],
   "source": [
    "# Compute the conditional entropy of the distribution\n",
    "\n",
    "p_Y = np.array([0.75, 0.25])\n",
    "h_Y = np.sum([-p*np.log2(p) for p in p_Y])\n",
    "h_X_givenY = h_XY - h_Y\n",
    "\n",
    "print(h_X_givenY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information.\n",
    "\n",
    "Let us return to the example of two random variables Z and Y. The random variable Z can be +1 or -1 with equal probability. On the other hand, the random variable Y can be 1 or 2 with equal probability. Now, define a new random variable  X= Y*Z\n",
    "What is the mutual information of X and Y in bits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "-0.18872187554086706\n"
     ]
    }
   ],
   "source": [
    "# Compute the mutual information of X and Y\n",
    "## I2(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
    "\n",
    "p_X = np.array([0.5, 0.5])\n",
    "p_XY = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "h_XY = np.sum([-p * np.log2(p) for p in p_XY if p != 0])\n",
    "\n",
    "print(h_XY)\n",
    "h_X = np.sum([-p*np.log2(p) for p in p_X])\n",
    "h_Y = np.sum([-p*np.log2(p) for p in p_Y])\n",
    "h_X_givenY = h_XY - h_Y\n",
    "I2_XY = h_X - h_X_givenY\n",
    "\n",
    "print(I2_XY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('mitx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be4cf647e822bd8704ca03d191902b0b63cac5adcb88b77eb022afc2b53edeb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
