{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excercise 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def IdentityMatrix(n):\n",
    "    imat = []\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            if r==c:\n",
    "                row.append(1)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        imat.append(row)\n",
    "    return imat\n",
    "    \n",
    "def DiagonalMatrix(n,l):\n",
    "    dmat = []\n",
    "    for r in range(n):\n",
    "        row = []\n",
    "        for c in range(n):\n",
    "            if r==c:\n",
    "                row.append(l)\n",
    "            else:\n",
    "                row.append(0)\n",
    "        dmat.append(row)\n",
    "    return dmat\n",
    "\n",
    "def H_Func(n):\n",
    "    one = np.ones(n)\n",
    "    i = IdentityMatrix(n)\n",
    "    H = np.subtract(i, np.dot(1/n, np.matmul(one, np.transpose(one))))\n",
    "    return H\n",
    "\n",
    "def gramMatrix(vectorList):\n",
    "    V = np.array(vectorList)\n",
    "    G = V.dot(V.T)\n",
    "    return G\n",
    "\n",
    "def eigenValues(vector):\n",
    "    w,v  = np.linalg.eig(vector)\n",
    "    return np.round(w,3)\n",
    "\n",
    "def eigenVector(vector):\n",
    "    w,v  = np.linalg.eig(vector)\n",
    "    return np.round(v,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the direction of the largest variance between two data points x1=[0,1] and x2=[0,-1], we need to calculate their covariance matrix first.\n",
    "\n",
    "The covariance matrix of two data points can be calculated as follows:\n",
    "\n",
    "Covariance matrix = [ cov(x1,x1)  cov(x1,x2) ]\n",
    "\n",
    "                    [ cov(x2,x1)  cov(x2,x2) ]\n",
    "\n",
    "where cov(x1,x2) represents the covariance between the two variables x1 and x2.\n",
    "\n",
    "For our data points x1=[0,1] and x2=[0,-1], we can see that the means of both variables are zero, so the covariance matrix can be simplified to:\n",
    "\n",
    "Covariance matrix = [ cov(x1,x1)  cov(x1,x2) ]\n",
    "                    [ cov(x2,x1)  cov(x2,x2) ]\n",
    "                  = [ 1  0 ]\n",
    "                    [ 0  1 ]\n",
    "The covariance matrix is a diagonal matrix, which means that the variables are uncorrelated. Since there is no covariance between the variables, the direction of the largest variance is simply the direction of the variable with the largest variance, which in this case is the y-axis or the direction of [0,1].\n",
    "\n",
    "\n",
    "### now compute the first principal component for the same\n",
    "\n",
    "To compute the first principal component for the data points x1=[0,1] and x2=[0,-1], we need to perform a principal component analysis (PCA) on the data.\n",
    "\n",
    "First, we can stack the two data points into a 2 x 2 matrix X:\n",
    "\n",
    "X = [0  1]\n",
    "    [0 -1]\n",
    "\n",
    "Next, we need to calculate the covariance matrix of X:\n",
    "\n",
    "\n",
    "Covariance matrix = [ cov(0,0)  cov(0,1) ]\n",
    "                    [ cov(1,0)  cov(1,-1) ]\n",
    "\n",
    "Since the means of both variables are zero, the covariance matrix can be simplified to:\n",
    "\n",
    "\n",
    "Covariance matrix = [ 0  0 ]\n",
    "                    [ 0  1 ]\n",
    "\n",
    "We can then find the eigenvectors and eigenvalues of the covariance matrix. The eigenvalues represent the amount of variance in each direction, and the eigenvectors represent the direction of that variance.\n",
    "\n",
    "To find the first principal component, we need to find the eigenvector corresponding to the largest eigenvalue. In this case, the largest eigenvalue is 1, and the corresponding eigenvector is [0,1].\n",
    "\n",
    "Therefore, the first principal component for the data points x1=[0,1] and x2=[0,-1] is the direction of the eigenvector [0,1].\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### what is the eigen values of the above covariance matrix\n",
    "\n",
    "The covariance matrix for the data points x1=[0,1] and x2=[0,-1] that we derived earlier is:\n",
    "\n",
    "Covariance matrix = [ 0  0 ]\n",
    "                    [ 0  1 ]\n",
    "\n",
    "To find the eigenvalues of this matrix, we need to solve the characteristic equation:\n",
    "\n",
    "\n",
    "det(Covariance matrix - λI) = 0\n",
    "\n",
    "where λ is the eigenvalue, I is the identity matrix, and det() represents the determinant.\n",
    "\n",
    "Substituting the covariance matrix, we get:\n",
    "\n",
    "\n",
    "det([0-λ   0  ]\n",
    "    [0     1-λ]) = 0\n",
    "\n",
    "Expanding the determinant, we get:\n",
    "\n",
    "(0-λ)(1-λ) - (0)(0) = 0\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "\n",
    "λ^2 - λ = 0\n",
    "\n",
    "Factoring out λ, we get:\n",
    "\n",
    "\n",
    "λ(λ - 1) = 0\n",
    "\n",
    "So the eigenvalues are λ = 0 and λ = 1.\n",
    "\n",
    "Therefore, the eigenvalues of the covariance matrix for the data points x1=[0,1] and x2=[0,-1] are 0 and 1, respectively. This means that the variance in the x-direction is zero (since the corresponding eigenvalue is 0), and the variance in the y-direction is 1 (since the corresponding eigenvalue is 1).\n",
    "\n",
    "\n",
    "### eigenvector for the above covariance matrix\n",
    "\n",
    "To find the eigenvectors of the covariance matrix for the data points x1=[0,1] and x2=[0,-1], we need to solve for the nullspace of the matrix (Covariance matrix - λI), where λ is the eigenvalue.\n",
    "\n",
    "For λ = 0, we have:\n",
    "\n",
    "(Covariance matrix - λI) * v = 0\n",
    "\n",
    "[0  0 ] * [v1]   [0]\n",
    "[0  1 ]   [v2] = [0]\n",
    "\n",
    "\n",
    "This gives us the equation 0*v1 + 0*v2 = 0, which means that v2 can take on any value. Therefore, the eigenvector for λ = 0 is [1, 0] (or any multiple of [1, 0]).\n",
    "\n",
    "For λ = 1, we have:\n",
    "\n",
    "(Covariance matrix - λI) * v = 0\n",
    "\n",
    "[-1  0 ] * [v1]   [0]\n",
    "[ 0  0 ]   [v2] = [0]\n",
    "\n",
    "This gives us the equation -v1 = 0, which means that v1 must be zero. Therefore, the eigenvector for λ = 1 is [0, 1] (or any multiple of [0, 1]).\n",
    "\n",
    "So the two eigenvectors for the covariance matrix are [1, 0] and [0, 1], corresponding to the eigenvalues 0 and 1, respectively. Note that these are also the basis vectors for the x- and y-axes, respectively, indicating that the covariance matrix is diagonal and the variables are uncorrelated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2928932188134525, 0.4142135623730951)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p12 = 1/(2+np.sqrt(2))\n",
    "p13 = p12\n",
    "p23 = np.sqrt(2)/(2+np.sqrt(2))\n",
    "p12,p23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[0,1] \n",
    "x2=[0,-1]\n",
    "Covariance_matrix = [[ np.cov(x1,x1),  np.cov(x1,x2) ],\n",
    "                    [ np.ov(x2,x1),  np.cov(x2,x2) ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gram matrix of the data points [1,1] [1,-1] [-1,1]\n",
    "\n",
    "G = [\n",
    "    [x1 . x1, x1 . x2, x1 . x3],\n",
    "    [x2 . x1, x2 . x2, x2 . x3],\n",
    "    [x3 . x1, x3 . x2, x3 . x3]\n",
    "    ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.]\n",
      " [-1.  0.]]\n",
      "gramMatrix = \n",
      " [[ 2  0  0]\n",
      " [ 0  2 -2]\n",
      " [ 0 -2  2]]\n",
      "\n",
      "eigenValues =  [4. 0. 2.]\n",
      "eigenVectors =  [[ 0.     0.     1.   ]\n",
      " [-0.707  0.707  0.   ]\n",
      " [ 0.707  0.707  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 = [1,1]\n",
    "x2 = [1,-1]\n",
    "x3 = [-1,1]\n",
    "\n",
    "x1t = np.dot(x1, np.transpose(x1))\n",
    "x2t = np.dot(x2, np.transpose(x2))\n",
    "x3t = np.dot(x3, np.transpose(x3))\n",
    "H =H_Func(2)\n",
    "print(H)\n",
    "gm = gramMatrix([x1,x2,x3])\n",
    "print(\"gramMatrix = \\n\",gm)\n",
    "\n",
    "print()\n",
    "print(\"eigenValues = \", eigenValues(gm))\n",
    "print(\"eigenVectors = \", eigenVector(gm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to perform spectral decomposition on the given Gram matrix G, we need to find its eigenvalues and eigenvectors.\n",
    "\n",
    "The eigenvalues of G are the solutions to the equation det(G - λI) = 0, where I is the identity matrix and det denotes the determinant.\n",
    "\n",
    "Substituting G, we get:\n",
    "\n",
    "det([\n",
    "    [2 - λ, 0, 0],\n",
    "    [0, 2 - λ, 0],\n",
    "    [0, 0, 2 - λ]\n",
    "    ]) = 0\n",
    "\n",
    "Expand the determinant,we get:\n",
    "\n",
    "(2 - λ) * (2 - λ) * (2 - λ) = 0\n",
    "\n",
    "Solving for λ, we get three eigenvalues: λ1 = 2, λ2 = 2, and λ3 = 2.\n",
    "\n",
    "To find the eigenvectors of G, we need to solve the equation Gv = λv, where v is a non-zero vector.\n",
    "\n",
    "For λ1 = 2, we have:\n",
    "\n",
    "[    [2 - λ1, 0, 0],\n",
    "    [0, 2 - λ1, 0],\n",
    "    [0, 0, 2 - λ1]\n",
    "] * [v1, v2, v3] = [0, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "Substituting λ1 and solving, we get:\n",
    "\n",
    "[    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "] * [v1, v2, v3] = [0, 0, 0]\n",
    "\n",
    "Since all the rows are zero, we can choose any non-zero value for v1, v2, and v3. For example, we can choose v1 = 1 and v2 = 0, and v3 = 0. Therefore, one eigenvector corresponding to λ1 = 2 is [1, 0, 0].\n",
    "\n",
    "For λ2 = 2, we have:\n",
    "\n",
    "\n",
    "[    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "] * [v1, v2, v3] = [0, 0, 0]\n",
    "\n",
    "Similarly, we can choose any non-zero value for v1, v2, and v3. For example, we can choose v1 = 0 and v2 = 1, and v3 = 0. Therefore, one eigenvector corresponding to λ2 = 2 is [0, 1, 0].\n",
    "\n",
    "For λ3 = 2, we have:\n",
    "\n",
    "\n",
    "[    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "] * [v1, v2, v3] = [0, 0, 0]\n",
    "\n",
    "Again, we can choose any non-zero value for v1, v2, and v3. For example, we can choose v1 = 0 and v2 = 0, and v3 = 1. Therefore, one eigenvector corresponding to λ3 = 2 is [0, 0, 1].\n",
    "\n",
    "Therefore, the spectral decomposition of the Gram matrix G is:\n",
    "\n",
    "\n",
    "G = λ1 * v1 *\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_mat = DiagonalMatrix(3,4)\n",
    "V = eigenVector(gm)\n",
    "np.var([[0,1],[0,-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1.],\n",
       "       [-1.,  0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_Func(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('LD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31c4a5216d1b70edb8ccadb0033ed466ad602c2eadb1aba86d4896a8d7ddf022"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
